name: Scrape Ukraine Articles

on:
  schedule:
    # Runs every 12 hours at 00:00 and 12:00 UTC
    # Note: Cron times are in UTC. Adjust if you need a specific timezone.
    - cron: '0 */12 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    # Only run if secrets are available (not on forks)
    if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
    
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'  # Cache pip dependencies for faster runs

      - name: üì¶ Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r scraper/requirements.txt

      - name: üåê Install Chrome and ChromeDriver
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
      
      - name: üîç Verify Chrome installation
        run: |
          echo "Chrome version:"
          google-chrome --version
          echo "ChromeDriver version:"
          chromedriver --version

      - name: üîç DNS Diagnostics
        run: |
          echo "Checking DNS resolution for database host..."
          echo "Host: ${{ secrets.DB_HOST }}"
          echo ""
          echo "IPv4 (A records):"
          getent ahosts "${{ secrets.DB_HOST }}" | grep STREAM | grep -v ':' || echo "No IPv4 addresses found"
          echo ""
          echo "IPv6 (AAAA records):"
          getent ahosts "${{ secrets.DB_HOST }}" | grep STREAM | grep ':' || echo "No IPv6 addresses found"
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
        continue-on-error: true

      - name: ü§ñ Run scraper
        env:
          DB_HOST: ${{ secrets.DB_HOST }}          # DNS hostname (for TLS verification)
          DB_HOSTADDR: ${{ secrets.DB_HOSTADDR }}  # IPv4 address (forces IPv4 connection)
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_SSLMODE: ${{ secrets.DB_SSLMODE || 'require' }}  # Default to 'require' for security
        run: python scraper/scraper.py
      
      - name: üìä Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            *.log
            /tmp/*.log
          retention-days: 7
          if-no-files-found: ignore
