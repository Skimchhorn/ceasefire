name: Scrape Ukraine Articles

# This workflow scrapes humanitarian articles from rescue.org
# and stores them in Supabase PostgreSQL database.
# 
# Required GitHub Secrets:
#   DB_HOST: Supabase pooler hostname (e.g., aws-1-ca-central-1.pooler.supabase.com)
#   DB_PORT: Database port (5432 for session pooler)
#   DB_NAME: Database name (usually 'postgres')
#   DB_USER: Database user (e.g., postgres.xxxxx)
#   DB_PASSWORD: Database password
#   DB_SSLMODE: SSL mode (optional, defaults to 'require')
#
# Note: Using Supabase Session Pooler for IPv4 compatibility with GitHub Actions

on:
  schedule:
    # Runs every 12 hours at 00:00 and 12:00 UTC
    # Note: Cron times are in UTC. Adjust if you need a specific timezone.
    - cron: '0 */12 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    # Only run if secrets are available (not on forks)
    if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
    
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'  # Cache pip dependencies for faster runs

      - name: üì¶ Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r scraper/requirements.txt

      - name: üåê Install Chrome and ChromeDriver
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
      
      - name: üîç Verify Chrome installation
        run: |
          echo "Chrome version:"
          google-chrome --version
          echo "ChromeDriver version:"
          chromedriver --version

      - name: üîç DNS Diagnostics (Supabase Pooler Check)
        run: |
          echo "Checking connection to Supabase pooler..."
          echo "Host: ${{ secrets.DB_HOST }}"
          echo ""
          if [[ "${{ secrets.DB_HOST }}" == *"pooler.supabase.com"* ]]; then
            echo "‚úÖ Using Supabase Session Pooler (IPv4 compatible)"
            echo "DNS lookup:"
            getent ahosts "${{ secrets.DB_HOST }}" | grep STREAM | head -3 || true
          else
            echo "‚ö†Ô∏è  Not using Supabase pooler"
            echo "IPv4 (A records):"
            getent ahosts "${{ secrets.DB_HOST }}" | grep STREAM | grep -v ':' || echo "No IPv4 addresses found"
            echo ""
            echo "IPv6 (AAAA records):"
            getent ahosts "${{ secrets.DB_HOST }}" | grep STREAM | grep ':' || echo "No IPv6 addresses found"
          fi
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
        continue-on-error: true

      - name: ü§ñ Run scraper
        env:
          DB_HOST: ${{ secrets.DB_HOST }}              # Supabase pooler hostname
          DB_PORT: ${{ secrets.DB_PORT }}              # Port (5432 for session pooler)
          DB_NAME: ${{ secrets.DB_NAME }}              # Database name
          DB_USER: ${{ secrets.DB_USER }}              # Database user
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}      # Database password
          DB_SSLMODE: ${{ secrets.DB_SSLMODE || 'require' }}  # SSL mode (default: require)
          # Note: DB_HOSTADDR not needed when using Supabase pooler (IPv4 built-in)
        run: python scraper/scraper.py
      
      - name: üìä Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            *.log
            /tmp/*.log
          retention-days: 7
          if-no-files-found: ignore
